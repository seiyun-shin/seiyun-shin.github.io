---
---
@article{hu2025efficient,
  title={Efficient data attribution via compresssed sensing (*equal contributions)},
  author={Hu*, Yuzheng and Shin*, Seiyun and Zhao, Han},
  journal={In preparation for the International Conference on Machine Learning (ICML)},
  volume={},
  selected = {true},
  year={2025}
}

@article{shin2025private,
  title={Efficient differential private sketching for covariance estimation},
  author={Shin, Seiyun and Bhattacharjee, Rajarshi and Musco, Cameron},
  journal={In preparation for the International Conference on Machine Learning (ICML)},
  volume={},
  selected = {true},
  year={2025}
}

@article{park2024transfer,
  title={Transfer learning in bandits with latent continuity},
  author={Park, Hyejin and Shin, Seiyun and Jun, Kwang-Sung and Ok, Jungseul},
  journal={IEEE Transactions on Information Theory},
  year={2024},
  selected = {true},
  publisher={IEEE}
}

@InProceedings{shin24_dynamidDBSCAN,
  title={Dynamic DBSCAN with Euler Tour Sequences},
  author={Shin, Seiyun and Shomorony, Ilan and Mcgregor, Peter},
  booktitle={submitted},
  volume={},
  number={},
  pages={},
  year={2025},
  month={},
  publisher={submitted to the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  selected = {true},
  abstract = {We propose a fast and dynamic algorithm for Density-Based Spatial Clustering of Applications with Noise (DBSCAN) that efficiently supports online updates.
Traditional DBSCAN algorithms, designed for batch processing, become computationally expensive when applied to dynamic datasets, particularly in large-scale applications where data continuously evolves.
To address this challenge, our algorithm leverages the Euler Tour Trees data structure, enabling dynamic clustering updates without the need to reprocess the entire dataset.
This approach preserves a near-optimal accuracy in density estimation, as achieved by the state-of-the-art static DBSCAN method~\citep{esfandiari2021almost}. 
Our method achieves an improved time complexity of $O(d \log^3(n) + \log^4(n))$ for every
data point insertion and deletion, where $n$ and $d$ denote the total number of updates and the data dimension, respectively.
Empirical studies also demonstrate significant speedups over conventional DBSCANs in real-time clustering of dynamic datasets, while maintaining comparable or superior clustering quality.}
}

@inproceedings{esfandiari2021almost,
  title={Almost linear time density level set estimation via dbscan},
  author={Esfandiari, Hossein and Mirrokni, Vahab and Zhong, Peilin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={8},
  pages={7349--7357},
  year={2021}
}

@InProceedings{shin23_graphnn,
  title={Efficient Learning of Linear Graph Neural Networks via Node Subsampling},
  author={Shin, Seiyun and Shomorony, Ilan and Zhao, Han},
  booktitle={Proceedings of the 37th Advances in Neural Information Processing Systems (NeurIPS)},
  volume={},
  number={},
  pages={},
  year={2023},
  month={Dec.},
  publisher={PMLR},
  pdf = 	 {https://proceedings.neurips.cc/paper_files/paper/2023/file/ada418ae9b6677dcda32d9dca0f7441f-Paper-Conference.pdf},
  url = 	 {https://proceedings.neurips.cc/paper_files/paper/2023/file/ada418ae9b6677dcda32d9dca0f7441f-Paper-Conference.html},
  selected = {true},
  abstract = {Graph Neural Networks (GNNs) are a powerful class of machine learning models with applications in recommender systems, drug discovery, social network analysis, and computer vision. One challenge with their implementation is that GNNs often take large-scale graphs as inputs, which imposes significant computational/storage costs in the training and testing phases. In particular, the message passing operations of a GNN require multiplication of the graph adjacency matrix $A \in \R^{n \times n}$ and the data matrix $X \in \R^{n \times d}$, and the $O(n^2 d)$ time complexity can be prohibitive for large $n$. Thus, a natural question is whether it is possible to perform the GNN operations in (quasi-)linear time by avoiding the full computation of $A X$. To study this question, we consider the setting of a regression task on a two-layer Linear Graph Convolutional Network (GCN). We develop an efficient training algorithm based on (1) performing node subsampling, (2) estimating the leverage scores of $A X$ based on the subsampled graph, and (3) performing leverage score sampling on $A X$. We show that our proposed scheme learns the regression model observing only $O(nd\eps^{-2}\log n)$ entries of $A$ in time $O(nd^2 \eps^{-2}\log n)$, with the guarantee that the learned weights deviate by at most $\epsilon$ under the $\ell_2$ norm from the model learned using the entire adjacency matrix $A$. We present empirical results for regression problems on two real-world graphs and show that our algorithm significantly outperforms other baseline sampling strategies that exploit the same number of observations.}
}

@InProceedings{shin23adaptive,
  title = 	 {{Adaptive Power Method: Eigenvector Estimation from Sampled Data}},
  author =       {Shin, Seiyun and Zhao, Han and Shomorony, Ilan},
  booktitle = 	 {Proceedings of The 34th International Conference on Algorithmic Learning Theory (ALT)},
  pages = 	 {1387--1410},
  year = 	 {2023},
  volume = 	 {201},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {Feb.},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v201/shin23a/shin23a.pdf},
  url = 	 {https://proceedings.mlr.press/v201/shin23a.html},
  selected = {true},
  abstract = 	 {Computing the dominant eigenvectors of a matrix $A$ has many applications, such as principal component analysis, spectral embedding, and PageRank. However, in general, this task relies on the complete knowledge of the matrix $A$, which can be too large to store or even infeasible to observe in many applications, e.g., large-scale social networks. Thus, a natural question is how to accurately estimate the eigenvectors of $A$ when only partial observations can be made by sampling entries from $A$. To this end, we propose the Adaptive Power Method (\textsc{APM}), a variant of the well-known power method. At each power iteration, \textsc{APM} adaptively selects a subset of the entries of $A$ to observe based on the current estimate of the top eigenvector. We show that \textsc{APM} can estimate the dominant eigenvector(s) of $A$ with squared error at most $\epsilon$ by observing roughly $O(n\epsilon^{-2} \log^2 (n/\epsilon))$ entries of an $n\times n$ matrix. We present empirical results for the problem of eigenvector centrality computation on two real-world graphs and show that \textsc{APM} significantly outperforms a non-adaptive estimation algorithm using the same number of observations. Furthermore, in the context of eigenvector centrality, \textsc{APM} can also adaptively allocate the observation budget to selectively refine the estimate of nodes with high centrality scores in the graph.}
}

@article{shin2021,
  title={Transfer Learning in Bandits with Latent Continuity},
  author={Park, Hyejin and Shin, Seiyun and Jun, Kwang-Sung and Ok, Jungseul},
  journal={IEEE Transactions on Information Theory},
  year={2024},
  pdf = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10633753},
  url = {https://ieeexplore.ieee.org/document/10633753},
  organization={IEEE},
  selected = {true},
  abstract = {Structured stochastic multi-armed bandits provide accelerated regret rates over the standard unstructured bandit problems. Most structured bandits, however, assume the knowledge of the structural parameter such as Lipschitz continuity, which is often not available. To cope with the latent structural parameter, we consider a transfer learning setting in which an agent must learn to transfer the structural information from the prior tasks to the next task, which is inspired by practical problems such as rate adaptation in wireless link. We propose a novel framework to provably and accurately estimate the Lipschitz constant based on previous tasks and fully exploit it for the new task at hand. We analyze the efficiency of the proposed framework in two folds: (i) the sample complexity of our estimator matches with the information-theoretic fundamental limit; and (ii) our regret bound on the new task is close to that of the oracle algorithm with the full knowledge of the Lipschitz constant under mild assumptions. Our analysis reveals a set of useful insights on transfer learning for latent Lipschitz constants such as the fundamental challenge a learner faces. Our numerical evaluations confirm our theoretical findings and show the superiority of the proposed framework compared to baselines.}
}


@inproceedings{park2021transfer,
  title={Transfer Learning in Bandits with Latent Continuity},
  author={Park, Hyejin and Shin, Seiyun and Jun, Kwang-Sung and Ok, Jungseul},
  booktitle={2021 IEEE International Symposium on Information Theory (ISIT)},
  pages={1463--1468},
  year={2021},
  pdf = {https://arxiv.org/abs/2102.02472.pdf},
  url = {https://arxiv.org/abs/2102.02472},
  organization={IEEE},
  selected = {true},
  abstract = {Structured stochastic multi-armed bandits provide accelerated regret rates over the standard unstructured bandit problems. Most structured bandits, however, assume the knowledge of the structural parameter such as Lipschitz continuity, which is often not available. To cope with the latent structural parameter, we consider a transfer learning setting in which an agent must learn to transfer the structural information from the prior tasks to the next task, which is inspired by practical problems such as rate adaptation in wireless link. We propose a novel framework to provably and accurately estimate the Lipschitz constant based on previous tasks and fully exploit it for the new task at hand. We analyze the efficiency of the proposed framework in two folds: (i) the sample complexity of our estimator matches with the information-theoretic fundamental limit; and (ii) our regret bound on the new task is close to that of the oracle algorithm with the full knowledge of the Lipschitz constant under mild assumptions. Our analysis reveals a set of useful insights on transfer learning for latent Lipschitz constants such as the fundamental challenge a learner faces. Our numerical evaluations confirm our theoretical findings and show the superiority of the proposed framework compared to baselines.}
}

@inproceedings{shin2020capacity,
  title={Capacity of the Erasure Shuffling Channel},
  author={Shin, Seiyun and Heckel, Reinhard and Shomorony, Ilan},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8841--8845},
  year={2020},
  pdf = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9053486},
  url = 	 {https://ieeexplore.ieee.org/document/9053486},
  organization={IEEE},
  selected = {true},
  abstract = {Motivated by DNA-based data storage, we study the erasure shuffling channel. This channel takes as input multiple strings, which are passed through an erasure channel and then shuffled out of order. We show that the capacity of this channel, for a large set of channel parameters, is given by the capacity of the binary erasure channel, $C_{BEC}$, minus a term that captures the loss of ordering information due to shuffling.}
}


@article{shin2019two,
  title={Two-way Function Computation},
  author={Shin, Seiyun and Suh, Changho},
  journal={IEEE Transactions on Information Theory},
  volume={66},
  number={2},
  pages={813--834},
  year={2019},
  pdf = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8804224},
  url = {https://ieeexplore.ieee.org/document/8804224},
  abstract = {We explore the role of interaction for the problem of reliable computation over two-way multicast networks. Specifically we consider a four-node network in which two nodes wish to compute a modulo-sum of two independent Bernoulli sources generated from the other two, and a similar task is done in the other direction. The main contribution of this work lies in the characterization of the computation capacity region for a deterministic model of the network via a novel transmission scheme. One consequence of this result is that, not only we can get an interaction gain over the one-way non-feedback computation capacities, but also we can sometimes get all the way to perfect-feedback computation capacities simultaneously in both directions. This result draws a parallel with the recent result developed in the context of two-way interference channels.},
  publisher={IEEE},
  selected = {true}
}

@inproceedings{shin2017capacity,
  title={Capacity of a Two-way Function Multicast Channel},
  author={Shin, Seiyun and Suh, Changho},
  booktitle={2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={125--133},
  year={2017},
  organization={IEEE},
  pdf = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8262728},
  url = {https://ieeexplore.ieee.org/document/8262728},
  abstract = {We explore the role of interaction for the problem of reliable computation over two-way multicast networks. Specifically we consider a four-node network in which two nodes wish to compute a modulo-sum of two independent Bernoulli sources generated from the other two, and a similar task is done in the other direction. The main contribution of this work lies in the characterization of the computation capacity region for a deterministic model of the network via a novel transmission scheme. One consequence of this result is that not only we can get an interaction gain over the one-way non-feedback computation capacities, we can sometime get all the way to perfect-feedback computation capacities simultaneously in both directions. This result draws a parallel with the recent result developed in the context of two-way interference channels.}
}

@inproceedings{min2017system,
  title={System Level Simulation of mmWave based Mobile Xhaul Networks},
  author={Min, Kyungsik and Jung, Minchae and Shin, Seiyun and Kim, Seokki and Choi, Sooyong},
  booktitle={2017 IEEE 85th Vehicular Technology Conference (VTC Spring)},
  pages={1--5},
  year={2017},
  organization={IEEE}
}